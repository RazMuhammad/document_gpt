# -*- coding: utf-8 -*-
"""document_gpt.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C1J8uieI0PtmOB1V4E8HGpGNuKxem4kt
"""

import streamlit as st
from sentence_transformers import SentenceTransformer
import pinecone
import requests
from huggingface_hub import hf_hub_download
from sentence_transformers import SentenceTransformer
import os # import os to get API key from environment variables
import dotenv


# Initialize the model and Pinecone
embedder = SentenceTransformer("evilfreelancer/enbeddrus-v0.2")

# Use Pinecone class for initialization
pinecone_api_key = os.getenv("pinecone_api_key") # get API key from environment variable
print(pinecone_api_key)
pinecone_environment = 'us-east-1' # set Pinecone environment
pinecone_index_name = 'document-gpt-vector-database' # set Pinecone index name

pinecone_client = pinecone.Pinecone(api_key=pinecone_api_key, environment=pinecone_environment)
index = pinecone_client.Index(pinecone_index_name) # Initialize Pinecone index

claude_api_key = os.getenv('claude_ai_api_key')
claude_api_url = "https://console.anthropic.com/settings/keys"  # Example endpoint

# ... (rest of your code
# Streamlit UI for document upload and query input
st.title("Generative AI Document Search")
uploaded_file = st.file_uploader("Upload a document", type=["txt", "pdf", "docx"])

if uploaded_file:
    document_text = uploaded_file.read().decode('utf-8')
    st.write("Document content:", document_text)

    # Embed the document text
    doc_embedding = embedder.encode(document_text)

    # Store the embedding in Pinecone
    doc_id = "document_1"  # Unique ID for the document
    index.upsert([(doc_id, doc_embedding.tolist())])

    st.write("Document uploaded and embedded successfully.")

# User input for query
query = st.text_input("Enter your query")

if query:
    # Embed the query
    query_embedding = embedder.encode(query)

    # Search in Pinecone
    results = index.query(query_embedding.tolist(), top_k=3)

    # Retrieve the top matching document chunks
    matching_chunks = [result['values'] for result in results['matches']]

    # Prepare the prompt
    prompt = f"Based on the document: {matching_chunks} \nAnswer the query: {query}"

    # Generate response using Claude AI
    headers = {
        "Authorization": f"Bearer {claude_api_key}",
        "Content-Type": "application/json"
    }

    data = {
        "model": "claude-v1",  # Replace with the correct model name if different
        "prompt": prompt,
        "max_tokens": 200
    }

    response = requests.post(claude_api_url, headers=headers, json=data)
    response_text = response.json().get('completion', '')

    st.write("Response:", response_text)

# Streamlit app execution
# if __name__ == "__main__":
#     st.run()

